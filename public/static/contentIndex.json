{"index":{"slug":"index","filePath":"index.md","title":"nmapped-garden","links":["notes","tags","notes/Vectors","notes/Gradient-Descent","scratch/Why-Not"],"tags":[],"content":"Hello, I’m [redacted]\nWelcome to my digital garden - a place where I collect and connect my thoughts, notes, and discoveries. This is a space for exploration and learning, built with Quartz.\nWhat You’ll Find Here\nThis site contains my personal notes on various topics including:\n\nMachine Learning and Programming\nTechnical explorations\nLearning resources\nAnd other things that interest me\n\nExplore My Notes\nHere are some ways to navigate this digital garden:\n\nBrowse all notes\nExplore by tags\nUse the search function in the sidebar (ctrl+k / cmd+k)\n\nRecently Updated\n\nVectors - Brief description\nGradient Descent - Brief description\nWhy Not? - Brief description\n\nFeel free to explore and follow the connections between ideas. This garden is always growing and evolving, just like my thoughts."},"drawings/Understanding-GEMM":{"slug":"drawings/Understanding-GEMM","filePath":"drawings/Understanding GEMM.md","title":"Understanding GEMM","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nExcalidraw Data\nText Elements\nM x K and K x N \nfirst row of first matrix:\na b c … K \nafter first column it skips by 8 \nfirst column of second matrix\na\nb\nc\n…\nK \nafter the first row it skips by 8 \nk is the magic number here\nk let’s us go through a row in the first matrix\nk let’s us go through a column in the second matrix  \nwhen we move row by row we do ops col wise and\nwhen we move col by col we do ops row wise\nthink mat mul steps \nto do mat mul for output matrix element i,j\nwe take row i of mat 1 times column j of mat 2. "},"notes/C-Hexdump-Program":{"slug":"notes/C-Hexdump-Program","filePath":"notes/C Hexdump Program.md","title":"C Hexdump Program","links":["endianness","bitwise-ops-c","sign-extension","Sign-Extension","Endianness","Bitwise-Operations"],"tags":[],"content":"Intro\nThe goal is to create a hexdump in C. I’ll essentially be trying to display binary data in a human readable format.\nMore Context\n\nunsigned char for binary data. Why? Endianness, bitwise operations, sign extension issues.\nsize_t is an unsigned int type that is portable across architectures. it is usually used to represent the size of objects in memory e.g. arrays, string, memory allocations. unsigned int is 4 bytes on 32-bit systems and could still be 4 bits on 64-bit systems. size_t is guaranteed to be 4 bytes on 32-bit systems and 8 bytes on 64-bit systems. this is good for addressing memory larger than 4gb.\nThe fread(buffer, element size, no of elements, stream) function automatically moves the file pointer forward after reading a chunk of bytes. In the example when used in the while loop, it moves automatically to the start of the next 16-byte after reading a 16-byte chunk.\n\nCode\n#include &lt;stdio.h&gt;\n#include &lt;ctype.h&gt;\n \nvoid print_hex_ln(unsigned char* buffer, size_t bytes_read, size_t* offset) {\n    printf(&quot;%08zX  &quot;, *offset);  // print offset\n \n    // print hex values\n    for (size_t i = 0; i &lt; bytes_read; i++) {\n        printf(&quot;%02X &quot;, buffer[i]);\n    }\n \n    // print ascii representation\n    printf(&quot;\\t&quot;);\n    for (size_t i = 0; i &lt; bytes_read; i++) {\n        printf(&quot;%c&quot;, isprint(buffer[i]) ? buffer[i] : &#039;.&#039;);\n    }\n \n    printf(&quot;\\n&quot;);\n}\n \nint main(int argc, char** argv) {\n    if (argc &lt; 2) {\n        printf(&quot;usage: %s &lt;filename&gt;\\n&quot;, argv[0]);\n        return 1;\n    }\n \n    FILE* fp = fopen(argv[1], &quot;rb&quot;);\n    if (fp == NULL) {\n        printf(&quot;error: can&#039;t open file %s\\n&quot;, argv[1]);\n        return 1;\n    }\n \n    unsigned char buffer[16];\n    size_t bytes_read;\n    size_t offset = 0;\n \n    while ((bytes_read = fread(buffer, 1, sizeof(buffer), fp)) &gt; 0) {\n        print_hex_ln(buffer, bytes_read, &amp;offset);\n        offset += bytes_read;\n    }\n \n    fclose(fp);\n    return 0;\n}\nFurther  Reading\n\nSign Extension\nEndianness\nBitwise Operations\n"},"notes/Gradient-Descent":{"slug":"notes/Gradient-Descent","filePath":"notes/Gradient Descent.md","title":"Gradient Descent","links":["neural-networks","notes/Partial-Derivatives-in-Gradient-Descent"],"tags":[],"content":"Intro\nIt’s an optimization algorithm used to minimize functions. It’s used in deep learning, linear regression, and a lot of other machine learning models, even functions unrelated to machine learning.\nMore Context\nGradient descent generally consists of the following steps:\n\nStart with random parameter values\nEvaluate the gradient of function  (loss function in ML) w.r.t. each parameter.\nScale gradient by a small positive number (learning rate in ML).\nSubtract resulting value from original parameter value and reassign to parameter\nRepeat until subsequent parameter changes are minimal, function is below a certain threshold or maximum number of iterations is reached.\n\nExample Loss Function. X_i represents the i^{th} input feature, and \\theta represents the parameters of the model.\nJ(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(f(X_i, \\theta) - y_i)^2\nGradient Descent Update Function.\n\\theta_{new} = \\theta_{old} - \\alpha \\frac{\\partial J(\\theta_{old})}{\\partial \\theta_{old}}\nFurther  Reading\n\nneural-networks\nPartial Derivatives in Gradient Descent\n"},"notes/Learning-Goals":{"slug":"notes/Learning-Goals","filePath":"notes/Learning Goals.md","title":"Learning Goals","links":[],"tags":[],"content":"Machine Learning\n\n Gain grounded understanding of fundamental concepts\n Linear algebra\n Calculus\n Probability and statistics\n\nProgramming Language Theory\n\n Gain grounded understanding of fundamental concepts\n Grammars\n Types\n\nLow Level Programming\n\n C\n ASM\n Rust\n"},"notes/Linear-Transformation":{"slug":"notes/Linear-Transformation","filePath":"notes/Linear Transformation.md","title":"Linear Transformation","links":[],"tags":[],"content":""},"notes/Partial-Derivatives-in-Gradient-Descent":{"slug":"notes/Partial-Derivatives-in-Gradient-Descent","filePath":"notes/Partial Derivatives in Gradient Descent.md","title":"Partial Derivatives in Gradient Descent","links":["notes/Gradient-Descent","notes/Vectors","neural-networks"],"tags":[],"content":"Explanation\nLet’s say we have a model with parameters w_1,\\; w_2,\\; ...,\\; w_n, and a cost function J(w,b) that we want to minimize. For example, in linear regression:\nJ(w,b) = \\frac{1}{2m}\\sum_{i=0}^{m}(h(x_i) - y_i)^2\nwhere\n\nh(x) = w^Tx + b is the model’s prediction.\ny is the actual output.\nm is the number of training examples.\n\nThe gradient of J(w,b) is the vector of its partial derivatives:\n\\nabla J = [\\frac{\\partial J}{\\partial w_1}, \\frac{\\partial J}{\\partial w_2}, \\cdots, \\frac{\\partial J}{\\partial w_n}]\nEach partial derivative \\frac{\\partial J}{\\partial w_i} tells us how and by how much the loss function responds to changes in w_i.\n\nIf \\frac{\\partial J}{\\partial w_i} &gt; 0, increasing w_i increases the cost, so w_i should be reduced.\nIf \\frac{\\partial J}{\\partial w_i} &lt; 0, increasing w_i reduces the cost, so w_i should be increased.\n\nThe partial derivatives are components of the gradient vector. The gradient \\nabla J points in the direction of steepest descent. The rate of change of J in any direction is given by the dot product:\n\\nabla J \\cdot \\hat d = |\\nabla J| \\cos \\theta\nwhere \\theta is the angle between \\nabla J and \\hat d. If \\hat d is aligned with \\nabla J, then \\cos \\theta = 1, meaning we move in the direction of fastest descent. Moving in the opposite direction minimizes J.\nIf we have two weights w_1 and w_2 with our cost function J, and the bias b, we need to go in the opposite direction of  \\nabla J in order to minimize the function. We can express the function as a 4-dimensional graph with the axes being w_1, w_2, b, J. Each weight affects J differently, which is why we need partial derivatives to measure their individual contributions.\nAll parameters are updated simultaneously to ensure a consistent descent direction. Also, to reiterate, the gradient shows us the direction of steepest descent so moving in the opposite direction is what we want, which is why we subtract the gradient from the current value. We can scale the movement using the learning rate \\alpha.\nFurther  Reading\n\nGradient Descent\nVectors\nneural-networks\n"},"notes/Reinforcement-Learning":{"slug":"notes/Reinforcement-Learning","filePath":"notes/Reinforcement Learning.md","title":"Reinforcement Learning","links":[],"tags":[],"content":"Intro\nReinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The goal is to maximize a cumulative reward over time.\nKey Components:\nKey components of RL include:\n\nAgent: The decision maker.\nEnvironment: The world the agent interacts with.\nState (S): The current situation the agent is in.\nAction(A): The possible moves the agent can make.\nReward(R): A numerical value given as feedback.\nPolicy (\\pi): A strategy that defines which action to take in each state.\n\nProcess\n\nThe agent observes the current state.\nIt selects an action based on its policy\nThe environment responds, providing a new state and a reward.\nThe agent updates its knowledge based on the reward.\nRepeat 1-4 until a condition is met.\n\nFurther  Reading"},"notes/Scalar-Multiplication":{"slug":"notes/Scalar-Multiplication","filePath":"notes/Scalar Multiplication.md","title":"Scalar Multiplication","links":["notes/Vectors","notes/Vector-Addition"],"tags":[],"content":"Definition\nScalar multiplication is simply the multiplication of a vector by a scalar (number). Formally:\n\\mathbf v = c \\cdot \\mathbf u \\; where \\; \\mathbf u \\in \\mathbb R^n \\; and \\; c \\in \\mathbb R\nMore Context\nIt satisfies the distributive property:\nc(\\mathbf u + \\mathbf v) = c \\mathbf u + c \\mathbf v\nIt satisfies the associative property:\n(cd) \\mathbf v = c (d \\mathbf v)\\; where\\; c,d \\in R\nIt satisfies the Identity property:\n\\begin{align*}\n1 \\mathbf v &amp;= \\mathbf v\\\\\n0 \\mathbf v &amp;= 0\n\\end{align*}\nIt can also be represented graphically and using matrices.\nGraphical Representation\n\n\\mathbf v = 3\\mathbf u\nMatrix Representation\n\\mathbf v = 3 \\cdot \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\cdot 1 \\\\ 3 \\cdot 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}\nFurther  Reading\n\nVectors\nVector Addition\n"},"notes/Vector-Addition":{"slug":"notes/Vector-Addition","filePath":"notes/Vector Addition.md","title":"Vector Addition","links":["notes/Vectors","notes/Scalar-Multiplication"],"tags":[],"content":"Definition\nVector addition as the name suggests, is the addition of vectors. Formally:\n\\mathbf v = \\mathbf u + \\mathbf w \\; where \\; \\mathbf v, \\mathbf u, \\mathbf w \\in \\mathbb R^n\nMore Context\nIt is commutative and associative:\n\\begin{align*}\n\\mathbf u + \\mathbf v &amp;= \\mathbf v + \\mathbf u \\\\\n(\\mathbf v + \\mathbf u) + \\mathbf w &amp;= \\mathbf u + (\\mathbf v + \\mathbf w)\n\\end{align*}\nIt can also be represented graphically or using matrices.\nGraphical Vector Addition\n\n\n\\mathbf v is the sum of \\mathbf u and \\mathbf w. The general idea is that \\mathbf v should move the straight line distance of \\mathbf u and \\mathbf w as well as their net direction.\nMatrix Representation\n\\mathbf v  = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 2 + 1 \\\\ 1 + 3 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\nFurther Reading\n\nVectors\nScalar Multiplication\n"},"notes/Vectors":{"slug":"notes/Vectors","filePath":"notes/Vectors.md","title":"Vectors","links":["notes/Vector-Addition","notes/Scalar-Multiplication"],"tags":[],"content":"Definition\nA vector can be defined in two ways. A vector can be defined as an arrow in  with a given length and a given direction. It could also be defined as an ordered list of numbers. How we choose to define a vector doesn’t matter as much as the relationship between these two views and how we translate from one representation to another.\nMore Context\nVectors as Arrows in Space\n\nVectors as Matrices (List of Numbers)\n\\mathbf u = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix},\n\\mathbf v = \\begin{bmatrix} -2 \\\\ 2 \\end{bmatrix}\nKey Points\n\nTwo fundamental operations are carried out on vectors: vector addition and scalar multiplication.\nA linear combination of one or more vectors is an expression where each vector is multiplied by a scalar and then subbed together. Formally:\n\nc \\mathbf{u} + d \\mathbf{v} + e\\mathbf{w}\nwhere \\mathbf u, \\mathbf v, \\mathbf w are vectors and c, d, e are scalars.\n\nA vector space is a set of vectors.\nThe span of one or more vectors are all the vectors that can be produced by the linear combination of those vectors.\n\nspan(\\mathbf v_1, \\mathbf v_2, ..., \\mathbf v_n) = c_1 \\mathbf v_1 + c_2 \\mathbf v_2 + ... + c_n \\mathbf v_n \\; | \\; c_1, c_2, ..., c_n \\in \\mathbb R\n\nA vector is linearly dependent on other vectors if it can be expressed as a linear combination of those vectors. Otherwise, it is linearly independent.\nA basis of a vector space is a set of linearly independent vectors that span the space.\n\nFurther Reading\n\nVector Addition\nScalar Multiplication\n"},"scratch/Intro-to-ML":{"slug":"scratch/Intro-to-ML","filePath":"scratch/Intro to ML.md","title":"Intro to ML","links":["scratch/Linear-Regression"],"tags":[],"content":"Machine Learning is the process of enabling computers to learn from data and make decisions or predictions without being explicitly programmed to do so.\nSome common types include supervised and unsupervised learning.\nSupervised learning algorithms learn to predict outputs from inputs or predict x to y mappings. It involves giving the learning algorithm the right answer y for each example to learn from.\nCommon supervised learning types include regression and classification. Regression involves predicting a single number from infinitely possible numbers while classification involves predicting a small number of possible outputs/categories.\nPredicting house prices based on house size is an example of a regression problem. Predicting if a tumor is malignant or benign is an example of a classification problem.\nUnsupervised learning requires the algorithm to find patterns in unlabeled data. It is used to find structure in data. The data has inputs x but no y.\nA type of unsupervised learning is clustering. For example, in Google News, similar news articles are grouped into clusters. Other examples include clustering DNA microarrays and automatic market segmentation.\nOther supervised learning algorithms include anomaly detection (used in fraud detection) and dimensionality reduction.\nFurther  Reading\n\nLinear Regression\n"},"scratch/Leisure":{"slug":"scratch/Leisure","filePath":"scratch/Leisure.md","title":"Leisure","links":[],"tags":[],"content":"Movies\n\nMegamind\nRise of the Guardians\nMaya and the Three\n\nAnime\n\nBoruto\nBleach\n"},"scratch/Linear-Regression":{"slug":"scratch/Linear-Regression","filePath":"scratch/Linear Regression.md","title":"Linear Regression","links":[],"tags":[],"content":"Intro\nA linear regression model can be described using this function:\nf_{w,b}(x) = w \\cdot x + b\nw and b are model parameters.\nw is a coefficient assigned to each input feature; it determines the influence of each feature on the predicted output. b is a constant term added to the model’s output. It allows the model to better fit the data by moving the regression line up or down.\nLinear regression with one input variable is called univariate or simple linear regression (single feature x). For example, when trying to predict house prices based on size, it’s still univariate linear regression because even though there is a list of sizes and prices i.e (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)},...,(x^{(n)},y^{(n)}), only one feature (size) is being used to determine the price. Each observation consists of a single size value and its corresponding price, and the model learns the relationship between these two. If each observation consisted of a size value and  something like distance to city center, then the regression would be multiple linear regression.\nSome helpful notations include:\n\nx: input feature\ny: target output\n(x^{(i)}, y^{(i)}): i^{th} training example\nm: number of training examples\n\\hat y: output prediction\n\nCost Function\nThis function tells us how well the model is doing. There are different functions used for this purpose but for linear regression the most common is the Mean Squared Error (MSE) function. This cost function can be expressed as:\nJ(w,b) = \\frac{1}{2m}\\sum_{i=1}^m(f_{w,b}(x^{(i)}) - y^{(i)})^2\nFor simplicity’s sake:\nJ(w,b) = \\frac{1}{2m}\\sum_{i=1}^m(\\hat y^{(i)} - y^{(i)} )^2\nThis function represents how much the prediction deviates from the correct value. The goal is to minimize it:\n\\underset{w,b}{minimize} \\; J(w,b)\nAn algorithm that allows us to automatically locate the minimum value of J(w,b) is called gradient descent.\nGradient Descent\nGradient descent is a method used to minimize functions in general, not just MSE.\nOutline:\n\nStart with some w,b\nKeep changing w,b to reduce J(w,b)\nUntil we settle at or near a minimum\n\nMathematically:\nw = w - \\alpha \\frac{\\partial}{\\partial w}J(w,b)\nb = b - \\alpha \\frac{\\partial}{\\partial b}J(w,b)\nfor each iteration of gradient descent.\n\n\\alpha is the learning rate. it determines how big a step gradient descent takes in each iteration.\nw and b are updated simultaneously.\n\nUsing just one parameter w (for the sake of simplicity), gradient descent would look like this:\nIf the slope of the tangent to J(w) at the current value of w i.e the partial derivative is positive, then it means\nw = w - \\alpha \\cdot (positive \\; number)\nwhich would lead to a decrease in the value of w. And if the slope at the value of w is negative, then it means\nw = w - \\alpha \\cdot (negative \\; number)\nwhich would lead to an increase in the value of w. When a minimum is reached, the slope of the tangent to that point is 0 which means:\nw = w - \\alpha \\cdot 0\nThis explains why gradient descent can reach a local minimum even with a fixed learning rate. With each iteration, the derivative becomes smaller in magnitude which means each step gets smaller and smaller. Doing gradient descent this way where each step of gradient descent uses all the training examples, is called “Batch” gradient descent."},"scratch/Multiple-Linear-Regression":{"slug":"scratch/Multiple-Linear-Regression","filePath":"scratch/Multiple Linear Regression.md","title":"Multiple Linear Regression","links":["scratch/Vectorized-Dot-Product-in-C"],"tags":[],"content":"Intro\nWith regular linear regression there is a single input, x used to predict a single output, y. With multiple linear regression, multiple inputs, \\vec x are used to predict y. Here are some additional notations:\n\nx_j: j^{th} feature\nn: number of features\n\\vec x^{(i)}: features of i^{th} training example\nx_j^{(i)}: value of feature j in i^{th} training example\n\nModel definition for multiple linear regression:\nf_{w,b}(x) = w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\cdots + w_n \\cdot x_n + b\nor\nf_{w,b}(\\vec x)=\\vec w \\cdot \\vec x + b\nVectorization\nWriting vectorized code allows you to take advantage of modern hardware to perform operations in parallel. It makes use of SIMD (Simple Instruction Multiple Data) which is a parallel processing technique that allows you to perform a single instruction on multiple data points simultaneously. Vectorized code is faster and more efficient.\nFurther Reading\n\nVectorized Dot Product in C\n"},"scratch/On-The-Issue-of-Prayer":{"slug":"scratch/On-The-Issue-of-Prayer","filePath":"scratch/On The Issue of Prayer.md","title":"On The Issue of Prayer","links":[],"tags":[],"content":"\nIf we pray for God’s will to be done, even though it will come to pass no matter what, then what is the true purpose of prayer? What is it’s true nature? What really is it?\n\n"},"scratch/Untitled":{"slug":"scratch/Untitled","filePath":"scratch/Untitled.md","title":"Untitled","links":[],"tags":[],"content":"For Ezekiel:\nGod is against it and “pronounced them unclean” because of it. We also see that God first gave them statutes “by which, if a man observes them, he will live.” This is what God wanted for them—life. But they rejected those statutes, returning to idols; and attached to those idols were statutes of death, by which they defiled themselves. They would not have statutes of life? Then, God says, here are your idols’ statues of death. God gave them over to these statutes of death so He could judge them for their evil through their own actions, showing that He is Lord and judgment awaits those who reject His life-giving commands. Romans 1:18–29 gives some insight. The “giving over” of cultures to do evil is part of God’s judgment. Just as God did not command the people of Romans 1 to murder, so He did not command the Israelites to burn their children in the fire. But He gave both over to the evil they desired so that “they might know that He is the Lord” through His judgment of that evil. Since His judgment proves He is against that evil, ironically, the Ezekiel passage is merely more evidence of God’s hatred of child sacrifice.\nFor the Aqedah:\nIt was a test of Abraham’s devotion to God.\nJudges:\nNot only does Jephthah know the Law, but it seems like he raised his daughter to know the Law and fear the Lord. After finding out about her father’s vow, she says, “My father, you have given your word to the Lord; do to me as you have said, since the Lord has avenged you of your enemies, the sons of Ammon” (Judg. 11:36). Since Jephthah knew the Law of Moses, and even raised his daughter to know it, it seems odd he would disregard all the passages forbidding child sacrifice (Lev. 18:21, 20:3; Deut. 12:31, 18:10) and disregard the fact that the penalty for child sacrifice was death by stoning (Lev. 20:2). It is also odd Jephthah would disobey a clear command of God so he could obey a command of God.\nThe second clue can be found in the vow Jephthah made. In Hebrew, the letter Vav is used as a conjunction joining two parts of speech. In English, we have many different words that function as conjunctions. Not so in Hebrew. The one letter Vav represents multiple different conjunctions. It can mean “and,” “together with,” “but,” “so,” “then,” and “or.” Many scholars have translated this Vav conjunction as “or.”\nIf that’s the case, then Jephthah’s vow says, “Whatever comes out of the doors of my house…it shall be the LORD’s, or I will offer it up as a burnt offering” (Judg. 11:31). This vow could be interpreted to mean that if the first thing to come out of the house was appropriate to offer as a burnt offering, then Jephthah would offer it as a burnt sacrifice. However, if the first thing to come out of the house wasn’t appropriate to offer as a burnt sacrifice, “it shall be the Lord’s,” meaning it would be dedicated to the Lord.\nThe third clue helps us to clarify whether Jephthah’s daughter was offered as a burnt sacrifice or dedicated to the Lord. When she heard the vow her father made, Jephthah’s daughter asked to take two months to mourn her virginity. This is odd if she was going to be killed. Why not mourn her impending death? Why not marry a man for two months?\nIt seems plausible from the fact that Jephthah’s daughter mourned her virginity that she was going to be dedicated to the Lord and live as a perpetual virgin. This idea is also reinforced by how the text emphasizes her virginity as part of her father’s vow. “At the end of two months she returned to her father, who did to her according to the vow which he had made; and she had no relations with a man” (Judg. 11:39). This would have been a tremendous sacrifice on Jephthah’s part, considering he only had one child. By dedicating his daughter to the Lord, Jephthah wouldn’t have grandchildren to carry on his lineage. We see Hannah make a similar sacrificial vow to dedicate her child Samuel to the Lord (1 Sam. 1).\nMicah:\nHe asks rhetorical questions exploring the question of what the LORD requires ultimately answering and making the point that God doesn’t want sacrifices for the sake of sacrifices but rather righteous character.\nExodus:\nFirst, Exodus 22:29–30. In order to understand what is meant by “give to me” in reference to firstborn sons, we need to look earlier in the book of Exodus where God gives the specifics of how this is to be done, for Exodus 22:29–30 only addresses when the command is to be carried out; it doesn’t contain the instructions on how to do it. For the instructions, we need to go back to Exodus 13:12–13\nI don’t know how that could be clearer (and it’s repeated again in Exodus 34:20: “You shall redeem all the firstborn of your sons”). They are to sacrifice the animals, but redeem the sons. It helps to understand why God commanded this, which is explained in the verses immediately following, Exodus 13:14–15\nThese verses tie the firstborn-son command to God’s rescue of the Israelites in the Exodus. God was merciful to the firstborn of the Israelites by not destroying them along with the firstborn of the Egyptians, and as a result, now they all belong to Him. Exodus 13:1–2, the introduction to this chapter, explains:\nThen the Lord spoke to Moses, saying, “Sanctify to Me every firstborn, the first offspring of every womb among the sons of Israel, both of man and beast; it belongs to Me.”\nAnd just as God redeemed His sons from the death of the firstborn, so the Israelites are to redeem their sons. Chapter 13 sets all of this up immediately after the Israelites begin the Exodus. Child sacrifice fits neither with the specifics of these instructions, nor with the practice’s overall purpose of serving as a continual visual reminder of God’s redemption of the firstborn in the Exodus. If the Israelites had killed their firstborn, they would have been identifying their sons with the Egyptian firstborn who were killed under God’s judgment. That would obviously go against God’s stated purpose for commanding this in the first place."},"scratch/Vectorized-Dot-Product-in-C":{"slug":"scratch/Vectorized-Dot-Product-in-C","filePath":"scratch/Vectorized Dot Product in C.md","title":"Vectorized Dot Product in C","links":[],"tags":[],"content":"Creating a microkernel (or \\mu-kernel) to compute the dot product efficiently using SIMD (Simple Instruction, Multiple Data).\nIn SIMD programming, a microkernel is a small, highly optimized compute kernel that handles a core computational task (dot product, matrix multiply, or a small tile of a larger matrix operation). It’s designed to run in tight loops, maximize use of CPU registers, SIMD instructions, and caches, be reused/composed into bigger kernels. In this context it’s just a small function/routine that performs a specific computation on a portion of data, usually in a data-parallel way. It’s often written to be run many times, one per element, or block of elements in a dataset.\ntl;dr: A microkernel is a small, focused, performance-critical function/routine that does the same operation on a bunch of data usually in parallel.\nA compute kernel is basically any function that performs a specific computation, usually parallelizable. This can be big or small e.g. a block of GEMM, or a full dot product.\nA microkernel is a small tightly optimized compute kernel, typically at the innermost loop level, tuned for a specific architecture. It’s a subset of compute kernels.\nAnother way to think of it is that a compute kernel is the general unit of computation while a microkernel is a special case; it’s the core, ultra-optimized tile inside that unit.\nSSE, AVX and AVX-512 are examples of SIMD instruction sets, extensions to x86 CPUs that allow multiple data elements to be processed in parallel using vector registers:\n\nSSE and NEON (ARM) 128-bit width, 4 floats at a time\nAVX has 256-bit width, 8 floats at a time\nAVX-512 has 512-bit width, 16 floats at a time\n\nVectorized Addition in C using SIMD:\n#include &lt;stdio.h&gt;\n#include &lt;immintrin.c&gt;\n \nvoid __simd_add(float *a, float *b, float *result) {\n \n\t/* load 8 floats from each array into the avx registers */\n\t__m256 vec_a = _mm256_loadu_ps(a);\n\t__m256 vec_b = _mm256_loadu_ps(b);\n \n\t/* add the vectors */\n\t__m256 vec_c = _mm256_add_ps(vec_a, vec_b);\n \n\t/* store the result in the result array */\n\t_m256_storeu_ps(result, vec_c);\n}\nVectorized Dot Product in C using SIMD:\n#include &lt;pmmintrin.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;immintrin.h&gt;\n#include &lt;xmmintrin.h&gt;\n \nfloat __simd_dot(const float *a, const float *b) {\n \n\t/* load 8 floats from the arrays */\n\t__m256 vec_a = _mm256_loadu_ps(a);\n\t__m256 vec_b = _mm256_loadu_ps(b);\n \n\t/* multiply the vectors */\n\t__m256 vec_mul = _mm256_mul_ps(vec_a, vec_b);\n \n\t/* get lower 128 bits (first 4 elements)*/\n\t__m128 low = _mm256_extractf128_ps(vec_mul, 0);\n\t/* get upper 128 bits (last 4 elements) */\n\t__m128 high = _mm256_extractf128_ps(vec_mul, 1);\n \n\t/* add low and high -&gt; [a, b, c, d]*/\n\t__m128 sum = _mm_add_ps(low, high);\n \n\t/* horizontally add elements of sum -&gt; [a+b, c+d]*/\n\tsum = _mm_hadd_ps(sum, sum);\n \n\t/* get the final product -&gt; [a+b + c+d]*/\n\tsum = _mm_hadd_ps(sum, sum);\n \n\t/* extract first element of __m128 register which holds the result */\n\treturn _mm_cvtss_f32(sum);\n}"}}